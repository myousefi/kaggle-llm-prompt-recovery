# Kaggle LLM Prompt Recovery

This repository contains code for the Kaggle competition "LLM Prompt Recovery". The goal of this competition is to develop models that can recover the original prompt given the response generated by a large language model (LLM).

## Project Structure

The project is organized as follows:

- `data/`: Contains the datasets used in the project
  - `external/`: Data from third party sources
  - `interim/`: Intermediate data that has been transformed  
  - `processed/`: Final datasets used for modeling
  - `raw/`: Original, immutable data dump
- `docs/`: Sphinx documentation 
- `models/`: Trained models, predictions, and summaries
- `notebooks/`: Jupyter notebooks for exploration and experimentation
  - Naming convention: `<step>-<initials>-<description>.ipynb` (e.g. `1.0-jqp-initial-data-exploration.ipynb`)
- `references/`: Data dictionaries, manuals, and reference material
- `reports/`: Generated analysis reports
  - `figures/`: Figures used in reporting
- `src/`: Source code
  - `data/`: Scripts for data processing 
    - `make_dataset.py`
  - `features/`: Scripts for feature engineering
    - `build_features.py`  
  - `models/`: Scripts for model training and prediction
    - `train_model.py`
    - `predict_model.py`
  - `visualization/`: Scripts for visualizations
    - `visualize.py`

Other key files:
- `LICENSE`: Project license
- `Makefile`: Useful commands via `make` 
- `README.md`: This file, a project overview
- `requirements.txt`: Lists required Python packages 
- `setup.py`: Package and distribution management
- `tox.ini`: tox configuration for standardized testing

## Getting Started

1. Clone the repository and navigate to the project directory
2. Create and activate a Python virtual environment
3. Install required packages: `pip install -r requirements.txt`
4. Run `make data` to fetch and preprocess the competition datasets
5. Explore the notebooks in `notebooks/` to understand the data and approaches
6. Extend the source code in `src/` to implement your solution
7. Use `make train` and `make predict` to train your model and generate predictions
8. Submit your predictions to Kaggle

## Modeling Approach

The basic modeling approach is:
1. Preprocess and clean the text data 
2. Extract relevant features from the prompt-response pairs
3. Train a supervised model to predict the original prompt from the response
4. Evaluate model performance on a holdout test set
5. Generate predictions for the Kaggle test set and submit

Refer to the `notebooks/` and `src/` directories for implementation details.

## Results


## References


## License


## Contact

Drop me a line at ## Modeling Approach

The basic modeling approach is:
1. Preprocess and clean the text data 
2. Extract relevant features from the prompt-response pairs
3. Train a supervised model to predict the original prompt from the response
4. Evaluate model performance on a holdout test set
5. Generate predictions for the Kaggle test set and submit

Refer to the `notebooks/` and `src/` directories for implementation details.

## Results


## References


## License


## Contact

## Contact

Drop me a line at [m@ysfi.me](mailto:m@ysfi.me?subject=Kaggle%20LLM%20Prompt%20Recovery%20REPO|)
